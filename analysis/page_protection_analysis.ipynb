{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Protection Analysis\n",
    "This is a rough analysis of how page protections (restrictions) changed after articles were posted to the Social Media Traffic Report (https://en.wikipedia.org/wiki/User:HostBot/Social_media_traffic_report). It is not meant to be fully robust (I would then pull a control sample of articles to compare to -- likely the articles that were just below the threshold to be posted to the report), but give a quick sense of the impact of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1004.eqiad.wmnet:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Jupyter Pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcc220b6ac8>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "TSV file of all of the unique page IDs that have shown up in the social media traffic report and the first date they appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>first_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>933889</td>\n",
       "      <td>2020_04_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>565250</td>\n",
       "      <td>2020_05_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2473996</td>\n",
       "      <td>2020_04_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>327694</td>\n",
       "      <td>2020_04_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28344340</td>\n",
       "      <td>2020_03_28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pageid  first_date\n",
       "0    933889  2020_04_15\n",
       "1    565250  2020_05_15\n",
       "2   2473996  2020_04_21\n",
       "3    327694  2020_04_24\n",
       "4  28344340  2020_03_28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageids_path = \"../social-media-traffic-reports/data/pageids_firstdate.tsv\"\n",
    "pageids_name = 'smtr_pageids'\n",
    "df = pd.read_csv(pageids_path, sep='\\t', header=None)\n",
    "df.columns = ['pageid', 'first_date']\n",
    "spark.createDataFrame(df).createOrReplaceTempView(pageids_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get page restrictions data via dumps (coarse look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 March 2020 dump\n",
    "# This is the closest dump snapshot to when the social media traffic report was launched\n",
    "pids = set(df['pageid'])\n",
    "page_restrictions_20200301 = {}\n",
    "prefix = 'INSERT INTO `page_restrictions` VALUES '\n",
    "postfix = ';\\n'\n",
    "with gzip.open('/mnt/data/xmldatadumps/public/enwiki/20200301/enwiki-20200301-page_restrictions.sql.gz', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        if line.startswith(prefix):\n",
    "            line = line[len(prefix):-len(postfix)].replace('NULL','None')\n",
    "            pages = eval(line)\n",
    "            for pr_page, pr_type, pr_level, _, _, pr_expiry, _ in pages:\n",
    "                if pr_page in pids:\n",
    "                    if pr_expiry != 'infinity' and int(pr_expiry) and pr_expiry < df[df['pageid'] == pr_page]['first_date'].values[0].replace('_', ''):\n",
    "                        continue\n",
    "                    if pr_page not in page_restrictions_20200301:\n",
    "                        page_restrictions_20200301[pr_page] = {}\n",
    "                    page_restrictions_20200301[pr_page][pr_type] = pr_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 May 2020 dump\n",
    "# This is the closest dump snapshot to the current day\n",
    "pids = set(df['pageid'])\n",
    "page_restrictions_20200501 = {}\n",
    "prefix = 'INSERT INTO `page_restrictions` VALUES '\n",
    "postfix = ';\\n'\n",
    "with gzip.open('/mnt/data/xmldatadumps/public/enwiki/20200501/enwiki-20200501-page_restrictions.sql.gz', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        if line.startswith(prefix):\n",
    "            line = line[len(prefix):-len(postfix)].replace('NULL','None')\n",
    "            pages = eval(line)\n",
    "            for pr_page, pr_type, pr_level, _, _, pr_expiry, _ in pages:\n",
    "                if pr_page in pids:\n",
    "                    if pr_expiry != 'infinity' and int(pr_expiry) and pr_expiry < df[df['pageid'] == pr_page]['first_date'].values[0].replace('_', ''):\n",
    "                        continue\n",
    "                    if pr_page not in page_restrictions_20200501:\n",
    "                        page_restrictions_20200501[pr_page] = {}\n",
    "                    page_restrictions_20200501[pr_page][pr_type] = pr_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoconfirmed        230\n",
      "extendedconfirmed      7\n",
      "sysop                  1\n",
      "Name: edit, dtype: int64\n",
      "\n",
      "sysop                181\n",
      "autoconfirmed        118\n",
      "extendedconfirmed      4\n",
      "Name: move, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# edit/move restrictions around start of the pilot\n",
    "at_start = pd.DataFrame(page_restrictions_20200301).T\n",
    "for c in at_start:\n",
    "    print(at_start[c].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoconfirmed        297\n",
      "extendedconfirmed     13\n",
      "sysop                  1\n",
      "Name: edit, dtype: int64\n",
      "\n",
      "sysop                186\n",
      "autoconfirmed        145\n",
      "extendedconfirmed      9\n",
      "Name: move, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# edit/move restrictions around end of the pilot\n",
    "at_end = pd.DataFrame(page_restrictions_20200501).T\n",
    "for c in at_end:\n",
    "    print(at_end[c].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get page restrictions data via logs (finer-grained look)\n",
    "Though this is still missing expirations, so it's actually hard to track what the page protections were right before an article was posted and in the weeks following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get page restriction changes since March 1st dump\n",
    "res = spark.sql(\"\"\"\n",
    "SELECT page_id,\n",
    "       meta.dt as log_timestamp,\n",
    "       page_restrictions\n",
    "  FROM event.mediawiki_page_restrictions_change pr\n",
    "  LEFT JOIN {0} pi\n",
    "       ON (pr.page_id = pi.pageid)\n",
    " WHERE year = 2020 and month >= 3 and `database` = 'enwiki'\n",
    " ORDER by page_id, log_timestamp LIMIT 100000\n",
    " \"\"\".format(pageids_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----------+\n",
      "|page_id|       log_timestamp|   page_restrictions|prior_state|\n",
      "+-------+--------------------+--------------------+-----------+\n",
      "|     12|2020-04-20T19:45:52Z|[edit -> autoconf...|         []|\n",
      "|    656|2020-03-20T18:52:47Z|[edit -> autoconf...|         []|\n",
      "|    803|2020-03-29T05:43:26Z|[edit -> autoconf...|         []|\n",
      "|   1009|2020-04-12T14:41:17Z|[edit -> autoconf...|         []|\n",
      "|   1130|2020-04-11T22:22:38Z|[edit -> autoconf...|         []|\n",
      "|   1130|2020-04-19T04:32:09Z|[edit -> autoconf...|         []|\n",
      "|   1175|2020-03-02T03:58:47Z|[edit -> autoconf...|         []|\n",
      "|   1182|2020-05-07T22:12:09Z|[edit -> autoconf...|         []|\n",
      "|   1348|2020-04-28T01:20:08Z|[edit -> autoconf...|         []|\n",
      "|   1530|2020-05-04T22:19:30Z|[edit -> autoconf...|         []|\n",
      "|   1541|2020-03-18T13:36:24Z|[edit -> autoconf...|         []|\n",
      "|   1624|2020-03-09T01:04:46Z|[edit -> autoconf...|         []|\n",
      "|   1653|2020-04-30T01:49:27Z|[edit -> autoconf...|         []|\n",
      "|   1774|2020-03-13T06:40:53Z|[edit -> autoconf...|         []|\n",
      "|   1805|2020-03-27T17:36:36Z|[edit -> autoconf...|         []|\n",
      "|   2076|2020-04-10T07:40:18Z|[edit -> autoconf...|         []|\n",
      "|   2185|2020-03-21T21:39:31Z|[edit -> autoconf...|         []|\n",
      "|   2195|2020-04-19T12:56:39Z|[edit -> autoconf...|         []|\n",
      "|   2195|2020-04-19T12:56:46Z|[edit -> autoconf...|         []|\n",
      "|   2196|2020-04-12T08:02:40Z|[edit -> autoconf...|         []|\n",
      "|   2224|2020-04-08T12:23:10Z|[edit -> autoconf...|         []|\n",
      "|   2443|2020-03-17T10:55:40Z|[edit -> autoconf...|         []|\n",
      "|   2983|2020-03-23T09:47:29Z|[edit -> sysop, m...|         []|\n",
      "|   3408|2020-05-05T05:57:18Z|[edit -> sysop, m...|         []|\n",
      "|   3916|2020-04-25T22:23:02Z|[edit -> autoconf...|         []|\n",
      "|   3920|2020-04-23T08:47:24Z|[edit -> , move -> ]|         []|\n",
      "|   3968|2020-05-01T04:09:30Z|[edit -> autoconf...|         []|\n",
      "|   4031|2020-05-06T02:37:56Z|[edit -> autoconf...|         []|\n",
      "|   4157|2020-03-12T22:07:28Z|[edit -> autoconf...|         []|\n",
      "|   4333|2020-04-15T02:34:36Z|[edit -> autoconf...|         []|\n",
      "|   4480|2020-04-05T03:12:16Z|[edit -> autoconf...|         []|\n",
      "|   4487|2020-03-29T11:19:03Z|[edit -> autoconf...|         []|\n",
      "|   4650|2020-04-16T02:18:48Z|[edit -> autoconf...|         []|\n",
      "|   4650|2020-04-16T02:19:12Z|[edit -> sysop, m...|         []|\n",
      "|   4650|2020-04-16T02:58:32Z|[edit -> autoconf...|         []|\n",
      "|   4792|2020-04-05T17:39:24Z|[edit -> autoconf...|         []|\n",
      "|   5347|2020-05-13T23:19:48Z|[edit -> autoconf...|         []|\n",
      "|   5488|2020-03-29T15:12:12Z|[edit -> autoconf...|         []|\n",
      "|   5489|2020-03-03T23:11:19Z|[edit -> autoconf...|         []|\n",
      "|   5489|2020-04-26T03:11:14Z|[edit -> autoconf...|         []|\n",
      "|   5530|2020-03-17T12:45:48Z|[edit -> autoconf...|         []|\n",
      "|   5796|2020-04-17T22:48:56Z|[edit -> autoconf...|         []|\n",
      "|   5828|2020-03-29T03:40:46Z|[edit -> autoconf...|         []|\n",
      "|   6220|2020-05-17T03:25:07Z|[edit -> autoconf...|         []|\n",
      "|   6226|2020-03-06T14:53:21Z|[edit -> autoconf...|         []|\n",
      "|   6226|2020-04-30T15:00:40Z|[move -> sysop, e...|         []|\n",
      "|   6310|2020-03-12T15:45:42Z|[edit -> autoconf...|         []|\n",
      "|   6310|2020-03-27T23:23:06Z|[edit -> autoconf...|         []|\n",
      "|   6610|2020-03-17T18:49:39Z|[edit -> autoconf...|         []|\n",
      "|   7249|2020-04-11T11:37:11Z|[edit -> autoconf...|         []|\n",
      "|   7738|2020-05-15T23:30:57Z|[edit -> autoconf...|         []|\n",
      "|   7839|2020-03-13T19:49:50Z|[edit -> autoconf...|         []|\n",
      "|   7839|2020-03-24T07:54:47Z|[edit -> autoconf...|         []|\n",
      "|   8063|2020-03-02T17:04:47Z|[edit -> autoconf...|         []|\n",
      "|   8092|2020-04-30T01:51:36Z|[edit -> autoconf...|         []|\n",
      "|   8196|2020-04-23T10:02:39Z|[edit -> , move -> ]|         []|\n",
      "|   8412|2020-04-03T21:30:44Z|[edit -> autoconf...|         []|\n",
      "|   8716|2020-03-14T16:56:42Z|[edit -> autoconf...|         []|\n",
      "|   8716|2020-03-14T16:56:52Z|[edit -> autoconf...|         []|\n",
      "|   8778|2020-04-08T06:45:24Z|[edit -> autoconf...|         []|\n",
      "|   8860|2020-03-25T20:27:00Z|[edit -> autoconf...|         []|\n",
      "|   8997|2020-04-01T01:57:30Z|[edit -> autoconf...|         []|\n",
      "|   9146|2020-05-08T22:38:52Z|[edit -> autoconf...|         []|\n",
      "|   9506|2020-04-01T13:19:38Z|[edit -> autoconf...|         []|\n",
      "|   9540|2020-05-07T23:33:02Z|[edit -> autoconf...|         []|\n",
      "|   9662|2020-03-07T19:39:50Z|[edit -> sysop, m...|         []|\n",
      "|   9662|2020-03-08T03:20:35Z|[edit -> , move -> ]|         []|\n",
      "|   9709|2020-03-13T01:21:07Z|[edit -> autoconf...|         []|\n",
      "|   9869|2020-03-01T19:02:25Z|[edit -> autoconf...|         []|\n",
      "|  10245|2020-05-12T10:34:50Z|[edit -> autoconf...|         []|\n",
      "|  10823|2020-04-07T14:56:32Z|[edit -> autoconf...|         []|\n",
      "|  10949|2020-03-15T22:11:14Z|[edit -> autoconf...|         []|\n",
      "|  10949|2020-03-15T22:11:44Z|[edit -> autoconf...|         []|\n",
      "|  11049|2020-04-21T16:21:22Z|[edit -> autoconf...|         []|\n",
      "|  11121|2020-03-05T18:23:31Z|[edit -> autoconf...|         []|\n",
      "|  11250|2020-05-04T11:58:13Z|[edit -> autoconf...|         []|\n",
      "|  11397|2020-05-04T02:11:26Z|[edit -> sysop, m...|         []|\n",
      "|  11508|2020-04-29T17:42:16Z|[edit -> , move -...|         []|\n",
      "|  11642|2020-04-09T02:04:12Z|[edit -> autoconf...|         []|\n",
      "|  11812|2020-04-09T02:02:16Z|[edit -> autoconf...|         []|\n",
      "|  11812|2020-05-09T04:59:51Z|[edit -> autoconf...|         []|\n",
      "|  12230|2020-04-27T06:44:31Z|[edit -> autoconf...|         []|\n",
      "|  12581|2020-04-23T02:04:14Z|[edit -> autoconf...|         []|\n",
      "|  12731|2020-03-15T02:40:44Z|[edit -> autoconf...|         []|\n",
      "|  12731|2020-05-13T20:33:27Z|[edit -> autoconf...|         []|\n",
      "|  13275|2020-03-31T16:33:59Z|[edit -> autoconf...|         []|\n",
      "|  13374|2020-05-09T05:17:13Z|[edit -> autoconf...|         []|\n",
      "|  13451|2020-05-13T02:27:34Z|[edit -> autoconf...|         []|\n",
      "|  13486|2020-03-24T13:19:02Z|[edit -> autoconf...|         []|\n",
      "|  13722|2020-04-28T14:06:09Z|[edit -> autoconf...|         []|\n",
      "|  13864|2020-03-16T20:40:57Z|[edit -> autoconf...|         []|\n",
      "|  13894|2020-03-20T09:36:35Z|[edit -> autoconf...|         []|\n",
      "|  14072|2020-05-16T17:18:17Z|[edit -> autoconf...|         []|\n",
      "|  14337|2020-05-16T23:15:04Z|[edit -> autoconf...|         []|\n",
      "|  14587|2020-04-08T10:42:30Z|[edit -> autoconf...|         []|\n",
      "|  14604|2020-05-13T10:18:11Z|[edit -> autoconf...|         []|\n",
      "|  14741|2020-03-16T19:12:07Z|[edit -> autoconf...|         []|\n",
      "|  14875|2020-03-12T22:50:25Z|[edit -> autoconf...|         []|\n",
      "|  15651|2020-03-15T23:06:14Z|[edit -> autoconf...|         []|\n",
      "|  15766|2020-04-19T10:07:02Z|[edit -> autoconf...|         []|\n",
      "+-------+--------------------+--------------------+-----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_restrictions_since_20200301 = res.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrictions_at_first_post(row):\n",
    "    # get best guess at restrictions right before when the article was posted\n",
    "    pid = row['pageid']\n",
    "    first_date = row['first_date'].replace('_', '')\n",
    "    restrictions = {}\n",
    "    if pid in page_restrictions_20200301:\n",
    "        restrictions = page_restrictions_20200301[pid]\n",
    "    updates = page_restrictions_since_20200301[page_restrictions_since_20200301['page_id'] == pid]\n",
    "    for i in range(len(updates)):\n",
    "        l = updates.iloc[i]\n",
    "        ts = l['log_timestamp'].replace('-', '')\n",
    "        ts = ts[:ts.index('T')]\n",
    "        if ts > first_date:\n",
    "            break\n",
    "        for k,v in l['page_restrictions'].items():\n",
    "            restrictions[k] = v\n",
    "    if restrictions:\n",
    "        return restrictions\n",
    "    return None\n",
    "\n",
    "def restrictions_after_first_post(row):\n",
    "    # get best guess at restrictions after the article was posted\n",
    "    # this should help fill in data post May 1st (last dump) and capture temporary protections\n",
    "    # that had expired before the May 1st dump\n",
    "    pid = row['pageid']\n",
    "    first_date = row['first_date'].replace('_', '')\n",
    "    restrictions = {}\n",
    "    if pid in page_restrictions_20200301:\n",
    "        restrictions = page_restrictions_20200301[pid]\n",
    "    updates = page_restrictions_since_20200301[page_restrictions_since_20200301['page_id'] == pid]\n",
    "    for i in range(len(updates)):\n",
    "        l = updates.iloc[i]\n",
    "        ts = l['log_timestamp'].replace('-', '')\n",
    "        ts = ts[:ts.index('T')]\n",
    "        for k,v in l['page_restrictions'].items():\n",
    "            restrictions[k] = v\n",
    "    if restrictions:\n",
    "        return restrictions\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['at_start'] = df.apply(lambda x: restrictions_at_first_post(x), axis=1)\n",
    "df['after_post'] = df.apply(lambda x: restrictions_after_first_post(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None                                                          2874\n",
       "{'edit': 'autoconfirmed', 'move': 'autoconfirmed'}             167\n",
       "{'edit': 'autoconfirmed', 'move': 'sysop'}                      97\n",
       "{'move': 'sysop'}                                               74\n",
       "{'edit': 'autoconfirmed', 'move': ''}                           51\n",
       "{'edit': 'autoconfirmed'}                                       25\n",
       "{'move': 'sysop', 'edit': 'autoconfirmed'}                      11\n",
       "{'edit': 'extendedconfirmed', 'move': 'extendedconfirmed'}      10\n",
       "{'move': 'autoconfirmed'}                                        4\n",
       "{'edit': 'sysop', 'move': 'sysop'}                               4\n",
       "{'edit': 'extendedconfirmed', 'move': 'sysop'}                   3\n",
       "{'move': 'autoconfirmed', 'edit': 'autoconfirmed'}               2\n",
       "{'edit': '', 'move': 'sysop'}                                    1\n",
       "{'edit': 'extendedconfirmed', 'move': ''}                        1\n",
       "Name: at_start, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['at_start'].apply(lambda x: str(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None                                                          2823\n",
       "{'edit': 'autoconfirmed', 'move': 'autoconfirmed'}             191\n",
       "{'edit': 'autoconfirmed', 'move': 'sysop'}                      99\n",
       "{'edit': 'autoconfirmed', 'move': ''}                           75\n",
       "{'move': 'sysop'}                                               74\n",
       "{'edit': 'autoconfirmed'}                                       25\n",
       "{'move': 'sysop', 'edit': 'autoconfirmed'}                      11\n",
       "{'edit': 'extendedconfirmed', 'move': 'extendedconfirmed'}      10\n",
       "{'move': 'autoconfirmed'}                                        4\n",
       "{'edit': 'sysop', 'move': 'sysop'}                               4\n",
       "{'edit': 'extendedconfirmed', 'move': 'sysop'}                   3\n",
       "{'move': 'autoconfirmed', 'edit': 'autoconfirmed'}               2\n",
       "{'edit': '', 'move': 'sysop'}                                    1\n",
       "{'edit': 'autoconfirmed', 'move': 'extendedconfirmed'}           1\n",
       "{'edit': 'extendedconfirmed', 'move': ''}                        1\n",
       "Name: after_post, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['after_post'].apply(lambda x: str(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3324 pages.\n",
      "3273 (0.985) pages had same protections.\n",
      "51 (0.015) pages had protections increased.\n",
      "0 (0.000) pages had protections decreased.\n"
     ]
    }
   ],
   "source": [
    "print(\"{0} pages.\".format(len(df)))\n",
    "protections_same = 0\n",
    "protections_increased = 0\n",
    "protections_decreased = 0\n",
    "for pid in pids:\n",
    "    at_start = df[df['pageid'] == pid]['at_start'].values[0]\n",
    "    after_post = df[df['pageid'] == pid]['after_post'].values[0]\n",
    "    if at_start is None and after_post is None:\n",
    "        protections_same += 1\n",
    "    elif at_start is None:\n",
    "        protections_increased += 1\n",
    "    elif after_post is None:\n",
    "        protections_decreased += 1\n",
    "    else:\n",
    "        if len(at_start) == len(after_post):\n",
    "            protections_same += 1\n",
    "        elif len(at_start) > len(after_post):\n",
    "            protections_decreased += 1\n",
    "        else:\n",
    "            protections_increased += 1\n",
    "print(\"{0} ({1:.3f}) pages had same protections.\".format(protections_same, protections_same / len(df)))\n",
    "print(\"{0} ({1:.3f}) pages had protections increased.\".format(protections_increased, protections_increased / len(df)))\n",
    "print(\"{0} ({1:.3f}) pages had protections decreased.\".format(protections_decreased, protections_decreased / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3324 pages.\n",
      "3260 (0.981) pages had no change.\n",
      "64 (0.019) pages had the following additions: {\"[('move', 'extendedconfirmed')]\": 1, \"[('move', 'sysop')]\": 2, \"[('edit', 'autoconfirmed'), ('move', 'autoconfirmed')]\": 24, \"[('move', '')]\": 4, \"[('move', 'autoconfirmed')]\": 6, \"[('edit', 'autoconfirmed'), ('move', '')]\": 27}\n",
      "13 (0.004) pages had the following removals: {\"[('move', '')]\": 7, \"[('move', 'autoconfirmed')]\": 6}\n"
     ]
    }
   ],
   "source": [
    "print(\"{0} pages.\".format(len(df)))\n",
    "added = {}\n",
    "removed = {}\n",
    "no_change = 0\n",
    "for pid in pids:\n",
    "    at_start = df[df['pageid'] == pid]['at_start'].values[0]\n",
    "    after_post = df[df['pageid'] == pid]['after_post'].values[0]\n",
    "    if at_start == after_post:\n",
    "        no_change += 1\n",
    "    else:\n",
    "        a = []\n",
    "        r = []\n",
    "        if at_start is None:\n",
    "            at_start = {}\n",
    "        if after_post is None:\n",
    "            after_post = {}\n",
    "        for p in at_start:\n",
    "            if at_start[p] != after_post.get(p, None):\n",
    "                r.append((p, at_start[p]))\n",
    "        r = sorted(r)\n",
    "        for p in after_post:\n",
    "            if after_post[p] != at_start.get(p, None):\n",
    "                a.append((p, after_post[p]))\n",
    "        a = sorted(a)\n",
    "        if r:\n",
    "            removed[str(r)] = removed.get(str(r), 0) + 1\n",
    "        if a:\n",
    "            added[str(a)] = added.get(str(a), 0) + 1\n",
    "\n",
    "print(\"{0} ({1:.3f}) pages had no change.\".format(no_change, no_change / len(df)))\n",
    "print(\"{0} ({1:.3f}) pages had the following additions: {2}\".format(sum(added.values()), sum(added.values()) / len(df), added))\n",
    "print(\"{0} ({1:.3f}) pages had the following removals: {2}\".format(sum(removed.values()), sum(removed.values()) / len(df), removed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
